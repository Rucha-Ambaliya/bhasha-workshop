{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8f437d",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2487411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Rucha_ws\\GitHub\\bhasha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137th Input sentence: ‡§á‡§® ‡§∏‡§¨ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§¶‡•á‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§æ‡§Ø‡•Å ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡§æ‡§Ü‡§µ‡§∏‡•ç‡§Ø‡§ï ‡§ú‡§ø‡§∏‡•á ‡§ú‡•à‡§∏‡•á, ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§° (CO‚ÇÇ), ‡§è‡§Ç‡§µ ‡§Ö‡§®‡•ç‡§Ø ‡§ö‡§ø‡§ú ‡§Ü ‡§ú‡§æ‡§§‡•Ä‡•§\n",
      "Tokens with language: [{'token': '‡§á‡§®', 'lang': 'hin'}, {'token': '‡§∏‡§¨', 'lang': 'hin'}, {'token': '‡§ï‡•á', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡§£', 'lang': 'hin'}, {'token': '‡§¶‡•á‡§ñ‡§æ', 'lang': 'hin'}, {'token': '‡§ó‡§Ø‡§æ', 'lang': 'hin'}, {'token': '‡§π‡•à', 'lang': 'hin'}, {'token': '‡§ï‡§ø', 'lang': 'hin'}, {'token': '‡§µ‡§æ‡§Ø‡•Å', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡§æ‡§Ü‡§µ‡§∏‡•ç‡§Ø‡§ï', 'lang': 'hin'}, {'token': '‡§ú‡§ø‡§∏‡•á', 'lang': 'hin'}, {'token': '‡§ú‡•à‡§∏‡•á', 'lang': 'hin'}, {'token': ',', 'lang': 'eng'}, {'token': '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡•ç‡§¨‡§®', 'lang': 'hin'}, {'token': '‡§°‡§æ‡§à', 'lang': 'hin'}, {'token': '‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§°', 'lang': 'hin'}, {'token': '(', 'lang': 'eng'}, {'token': 'CO‚ÇÇ', 'lang': 'eng'}, {'token': ')', 'lang': 'eng'}, {'token': ',', 'lang': 'eng'}, {'token': '‡§è‡§Ç‡§µ', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡•ç‡§Ø', 'lang': 'hin'}, {'token': '‡§ö‡§ø‡§ú', 'lang': 'hin'}, {'token': '‡§Ü', 'lang': 'hin'}, {'token': '‡§ú‡§æ‡§§‡•Ä', 'lang': 'hin'}, {'token': '‡•§', 'lang': 'hin'}]\n",
      "\n",
      "137th Output sentence: ‡§á‡§® ‡§∏‡§¨ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§¶‡•á‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§æ‡§Ø‡•Å ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡§æ‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§° (CO‚ÇÇ), ‡§è‡§µ‡§Ç ‡§Ö‡§®‡•ç‡§Ø ‡§µ‡§∏‡•ç‡§§‡•Å‡§Ø‡•á‡§Ç ‡§Ü ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\n",
      "Tokens with language: [{'token': '‡§á‡§®', 'lang': 'hin'}, {'token': '‡§∏‡§¨', 'lang': 'hin'}, {'token': '‡§ï‡•á', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡§£', 'lang': 'hin'}, {'token': '‡§¶‡•á‡§ñ‡§æ', 'lang': 'hin'}, {'token': '‡§ó‡§Ø‡§æ', 'lang': 'hin'}, {'token': '‡§π‡•à', 'lang': 'hin'}, {'token': '‡§ï‡§ø', 'lang': 'hin'}, {'token': '‡§µ‡§æ‡§Ø‡•Å', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡§æ‡§µ‡§∂‡•ç‡§Ø‡§ï', 'lang': 'hin'}, {'token': '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡•ç‡§¨‡§®', 'lang': 'hin'}, {'token': '‡§°‡§æ‡§à', 'lang': 'hin'}, {'token': '‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§°', 'lang': 'hin'}, {'token': '(', 'lang': 'eng'}, {'token': 'CO‚ÇÇ', 'lang': 'eng'}, {'token': ')', 'lang': 'eng'}, {'token': ',', 'lang': 'eng'}, {'token': '‡§è‡§µ‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡•ç‡§Ø', 'lang': 'hin'}, {'token': '‡§µ‡§∏‡•ç‡§§‡•Å‡§Ø‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ü', 'lang': 'hin'}, {'token': '‡§ú‡§æ‡§§‡•Ä', 'lang': 'hin'}, {'token': '‡§π‡•à‡§Ç', 'lang': 'hin'}, {'token': '‡•§', 'lang': 'hin'}]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Required imports\n",
    "# -------------------------------\n",
    "print(os.getcwd())\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "csv_path = \"./IndicGEC2025/Hindi/train.csv\"\n",
    "ssf_tokenizer_script = \"./Tokenizer_for_Indian_Languages/tokenize_in_SSF_format_with_sentence_tokenization.py\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load ILID model\n",
    "# -------------------------------\n",
    "tokenizer_model = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pruthwik/ilid-muril-model\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# ILID index to label mapping\n",
    "index_to_label_dict = {0: 'asm', 1: 'ben', 2: 'brx', 3: 'doi', 4: 'eng', 5: 'gom', \n",
    "                       6: 'guj', 7: 'hin', 8: 'kan', 9: 'kas', 10: 'mai', 11: 'mal', \n",
    "                       12: 'mar', 13: 'mni_Beng', 14: 'mni_Mtei', 15: 'npi', 16: 'ory', \n",
    "                       17: 'pan', 18: 'san', 19: 'sat', 20: 'snd_Arab', 21: 'snd_Deva', \n",
    "                       22: 'tam', 23: 'tel', 24: 'urd'}\n",
    "\n",
    "# -------------------------------\n",
    "# Function to tokenize a sentence using SSF tokenizer\n",
    "# -------------------------------\n",
    "def tokenize_sentence_ssf(sentence):\n",
    "    \"\"\"\n",
    "    Tokenize a single sentence using SSF tokenizer.\n",
    "    Returns a list of tokens (strings).\n",
    "    \"\"\"\n",
    "    with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(sentence.strip() + \"\\n\")\n",
    "    \n",
    "    # Run tokenizer script\n",
    "    subprocess.run(\n",
    "        [sys.executable, ssf_tokenizer_script, \"--input\", \"temp_input.txt\", \"--output\", \"temp_output.txt\", \"--lang\", \"hi\"],\n",
    "        check=True\n",
    "    )\n",
    "    \n",
    "    # Read tokens from output\n",
    "    tokens = []\n",
    "    with open(\"temp_output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"(\"):\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                tokens.append(parts[1])\n",
    "    return tokens\n",
    "\n",
    "# -------------------------------\n",
    "# Function to assign token-level language\n",
    "# -------------------------------\n",
    "def label_tokens(tokens, sentence_lang):\n",
    "    \"\"\"\n",
    "    Assign token-level language.\n",
    "    ASCII/English tokens -> 'eng'\n",
    "    Others -> sentence-level lang\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for t in tokens:\n",
    "        if any(c.isascii() for c in t):\n",
    "            labeled.append({\"token\": t, \"lang\": \"eng\"})\n",
    "        else:\n",
    "            labeled.append({\"token\": t, \"lang\": sentence_lang})\n",
    "    return labeled\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "input_sentences = df[\"Input sentence\"].tolist()\n",
    "output_sentences = df[\"Output sentence\"].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# Process sentences\n",
    "# -------------------------------\n",
    "input_tokens_list = []\n",
    "output_tokens_list = []\n",
    "\n",
    "for inp_sent, out_sent in zip(input_sentences, output_sentences):\n",
    "    # Predict sentence language\n",
    "    inp_pred = pipe([inp_sent])[0]['label']\n",
    "    inp_lang = index_to_label_dict[int(inp_pred.split('_')[1])]\n",
    "    \n",
    "    out_pred = pipe([out_sent])[0]['label']\n",
    "    out_lang = index_to_label_dict[int(out_pred.split('_')[1])]\n",
    "    \n",
    "    # Tokenize\n",
    "    inp_tokens = tokenize_sentence_ssf(inp_sent)\n",
    "    out_tokens = tokenize_sentence_ssf(out_sent)\n",
    "    \n",
    "    # Assign token-level language\n",
    "    input_tokens_list.append(label_tokens(inp_tokens, inp_lang))\n",
    "    output_tokens_list.append(label_tokens(out_tokens, out_lang))\n",
    "\n",
    "# -------------------------------\n",
    "# Example: row 137\n",
    "# -------------------------------\n",
    "row_index = 136  # 137th row (0-based index)\n",
    "print(\"137th Input sentence:\", input_sentences[row_index])\n",
    "print(\"Tokens with language:\", input_tokens_list[row_index])\n",
    "print(\"\\n137th Output sentence:\", output_sentences[row_index])\n",
    "print(\"Tokens with language:\", output_tokens_list[row_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0450de",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba70ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Imports\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "from indictrans import Transliterator  # Python class directly\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "csv_path = \"../IndicGEC2025/Hindi/train.csv\"\n",
    "ssf_tokenizer_script = \"../Tokenizer_for_Indian_Languages/tokenize_in_SSF_format_with_sentence_tokenization.py\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load ILID model for sentence-level language identification\n",
    "# -------------------------------\n",
    "tokenizer_model = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pruthwik/ilid-muril-model\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "index_to_label_dict = {\n",
    "    0: 'asm', 1: 'ben', 2: 'brx', 3: 'doi', 4: 'eng', 5: 'gom',\n",
    "    6: 'guj', 7: 'hin', 8: 'kan', 9: 'kas', 10: 'mai', 11: 'mal',\n",
    "    12: 'mar', 13: 'mni_Beng', 14: 'mni_Mtei', 15: 'npi', 16: 'ory',\n",
    "    17: 'pan', 18: 'san', 19: 'sat', 20: 'snd_Arab', 21: 'snd_Deva',\n",
    "    22: 'tam', 23: 'tel', 24: 'urd'\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenize a sentence using SSF tokenizer\n",
    "# -------------------------------\n",
    "def tokenize_sentence_ssf(sentence):\n",
    "    with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(sentence.strip() + \"\\n\")\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, ssf_tokenizer_script,\n",
    "         \"--input\", \"temp_input.txt\",\n",
    "         \"--output\", \"temp_output.txt\",\n",
    "         \"--lang\", \"hi\"],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    tokens = []\n",
    "    with open(\"temp_output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"(\"):\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                tokens.append(parts[1])\n",
    "    return tokens\n",
    "\n",
    "# -------------------------------\n",
    "# Transliteration helpers\n",
    "# -------------------------------\n",
    "trn_cache = {}   # cache Transliterator outputs\n",
    "trn_dict = {}    # one Transliterator per target language\n",
    "\n",
    "def get_transliterator(main_lang):\n",
    "    \"\"\"Return Transliterator object if supported, else None (fallback).\"\"\"\n",
    "    if main_lang in trn_dict:\n",
    "        return trn_dict[main_lang]\n",
    "\n",
    "    try:\n",
    "        trn = Transliterator(source='eng', target=main_lang, build_lookup=True)\n",
    "        trn_dict[main_lang] = trn\n",
    "        return trn\n",
    "    except NotImplementedError:\n",
    "        print(f\"[Warning] Transliteration not implemented for eng-{main_lang}. Using raw tokens.\")\n",
    "        trn_dict[main_lang] = None\n",
    "        return None\n",
    "\n",
    "def label_and_transliterate_tokens(tokens, main_lang):\n",
    "    labeled = []\n",
    "    trn = get_transliterator(main_lang)\n",
    "\n",
    "    for t in tokens:\n",
    "        # Punctuation\n",
    "        if all(char in '.,!?()[]{}:;\\'\"‚Äú‚Äù‚Äò‚Äô' for char in t):\n",
    "            labeled.append({\"token\": t, \"lang\": \"punct\"})\n",
    "        # English/ASCII -> transliterate (if available)\n",
    "        elif any(c.isascii() for c in t):\n",
    "            if trn is not None:\n",
    "                if t in trn_cache:\n",
    "                    translit = trn_cache[t]\n",
    "                else:\n",
    "                    translit = trn.transform(t)\n",
    "                    trn_cache[t] = translit\n",
    "                labeled.append({\"token\": translit, \"lang\": main_lang})\n",
    "            else:\n",
    "                # Fallback: keep token as-is\n",
    "                labeled.append({\"token\": t, \"lang\": \"eng\"})\n",
    "        # Non-English word -> keep as main language\n",
    "        else:\n",
    "            labeled.append({\"token\": t, \"lang\": main_lang})\n",
    "    return labeled\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "input_sentences = df[\"Input sentence\"].tolist()\n",
    "output_sentences = df[\"Output sentence\"].tolist()\n",
    "\n",
    "input_tokens_list = []\n",
    "output_tokens_list = []\n",
    "\n",
    "# -------------------------------\n",
    "# Process each sentence row\n",
    "# -------------------------------\n",
    "for inp_sent, out_sent in zip(input_sentences, output_sentences):\n",
    "    # Sentence-level language\n",
    "    inp_pred = pipe([inp_sent])[0]['label']\n",
    "    inp_lang = index_to_label_dict[int(inp_pred.split('_')[1])]\n",
    "\n",
    "    out_pred = pipe([out_sent])[0]['label']\n",
    "    out_lang = index_to_label_dict[int(out_pred.split('_')[1])]\n",
    "\n",
    "    # Tokenize\n",
    "    inp_tokens = tokenize_sentence_ssf(inp_sent)\n",
    "    out_tokens = tokenize_sentence_ssf(out_sent)\n",
    "\n",
    "    # Assign token-level language & transliterate\n",
    "    input_tokens_list.append(label_and_transliterate_tokens(inp_tokens, inp_lang))\n",
    "    output_tokens_list.append(label_and_transliterate_tokens(out_tokens, out_lang))\n",
    "\n",
    "# -------------------------------\n",
    "# Example row\n",
    "# -------------------------------\n",
    "row_index = 136\n",
    "print(\"137th Input sentence:\", input_sentences[row_index])\n",
    "print(\"Tokens with language:\", input_tokens_list[row_index])\n",
    "print(\"\\n137th Output sentence:\", output_sentences[row_index])\n",
    "print(\"Tokens with language:\", output_tokens_list[row_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ceb9b",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9810180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137th Input sentence: ‡§á‡§® ‡§∏‡§¨ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§¶‡•á‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§æ‡§Ø‡•Å ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡§æ‡§Ü‡§µ‡§∏‡•ç‡§Ø‡§ï ‡§ú‡§ø‡§∏‡•á ‡§ú‡•à‡§∏‡•á, ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§° (CO‚ÇÇ), ‡§è‡§Ç‡§µ ‡§Ö‡§®‡•ç‡§Ø ‡§ö‡§ø‡§ú ‡§Ü ‡§ú‡§æ‡§§‡•Ä‡•§\n",
      "Tokens with language: [{'token': '‡§á‡§®', 'lang': 'hin'}, {'token': '‡§∏‡§¨', 'lang': 'hin'}, {'token': '‡§ï‡•á', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡§£', 'lang': 'hin'}, {'token': '‡§¶‡•á‡§ñ‡§æ', 'lang': 'hin'}, {'token': '‡§ó‡§Ø‡§æ', 'lang': 'hin'}, {'token': '‡§π‡•à', 'lang': 'hin'}, {'token': '‡§ï‡§ø', 'lang': 'hin'}, {'token': '‡§µ‡§æ‡§Ø‡•Å', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡§æ‡§Ü‡§µ‡§∏‡•ç‡§Ø‡§ï', 'lang': 'hin'}, {'token': '‡§ú‡§ø‡§∏‡•á', 'lang': 'hin'}, {'token': '‡§ú‡•à‡§∏‡•á', 'lang': 'hin'}, {'token': ',', 'lang': 'punct'}, {'token': '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡•ç‡§¨‡§®', 'lang': 'hin'}, {'token': '‡§°‡§æ‡§à', 'lang': 'hin'}, {'token': '‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§°', 'lang': 'hin'}, {'token': '(', 'lang': 'punct'}, {'token': 'CO‚ÇÇ', 'lang': 'hin'}, {'token': ')', 'lang': 'punct'}, {'token': ',', 'lang': 'punct'}, {'token': '‡§è‡§Ç‡§µ', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡•ç‡§Ø', 'lang': 'hin'}, {'token': '‡§ö‡§ø‡§ú', 'lang': 'hin'}, {'token': '‡§Ü', 'lang': 'hin'}, {'token': '‡§ú‡§æ‡§§‡•Ä', 'lang': 'hin'}, {'token': '‡•§', 'lang': 'hin'}]\n",
      "\n",
      "137th Output sentence: ‡§á‡§® ‡§∏‡§¨ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§¶‡•á‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡§æ‡§Ø‡•Å ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡§æ‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§° (CO‚ÇÇ), ‡§è‡§µ‡§Ç ‡§Ö‡§®‡•ç‡§Ø ‡§µ‡§∏‡•ç‡§§‡•Å‡§Ø‡•á‡§Ç ‡§Ü ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\n",
      "Tokens with language: [{'token': '‡§á‡§®', 'lang': 'hin'}, {'token': '‡§∏‡§¨', 'lang': 'hin'}, {'token': '‡§ï‡•á', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡§£', 'lang': 'hin'}, {'token': '‡§¶‡•á‡§ñ‡§æ', 'lang': 'hin'}, {'token': '‡§ó‡§Ø‡§æ', 'lang': 'hin'}, {'token': '‡§π‡•à', 'lang': 'hin'}, {'token': '‡§ï‡§ø', 'lang': 'hin'}, {'token': '‡§µ‡§æ‡§Ø‡•Å', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡§æ‡§µ‡§∂‡•ç‡§Ø‡§ï', 'lang': 'hin'}, {'token': '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ', 'lang': 'hin'}, {'token': '‡§Æ‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§ï‡§æ‡§∞‡•ç‡§¨‡§®', 'lang': 'hin'}, {'token': '‡§°‡§æ‡§à', 'lang': 'hin'}, {'token': '‡§Ö‡§ï‡•ç‡§∏‡§æ‡§á‡§°', 'lang': 'hin'}, {'token': '(', 'lang': 'punct'}, {'token': 'CO‚ÇÇ', 'lang': 'hin'}, {'token': ')', 'lang': 'punct'}, {'token': ',', 'lang': 'punct'}, {'token': '‡§è‡§µ‡§Ç', 'lang': 'hin'}, {'token': '‡§Ö‡§®‡•ç‡§Ø', 'lang': 'hin'}, {'token': '‡§µ‡§∏‡•ç‡§§‡•Å‡§Ø‡•á‡§Ç', 'lang': 'hin'}, {'token': '‡§Ü', 'lang': 'hin'}, {'token': '‡§ú‡§æ‡§§‡•Ä', 'lang': 'hin'}, {'token': '‡§π‡•à‡§Ç', 'lang': 'hin'}, {'token': '‡•§', 'lang': 'hin'}]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Imports\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "from indictrans import Transliterator  # Python class directly\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "csv_path = \"../IndicGEC2025/Hindi/train.csv\"\n",
    "ssf_tokenizer_script = \"../Tokenizer_for_Indian_Languages/tokenize_in_SSF_format_with_sentence_tokenization.py\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load ILID model for sentence-level language identification\n",
    "# -------------------------------\n",
    "tokenizer_model = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pruthwik/ilid-muril-model\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "index_to_label_dict = {\n",
    "    0: 'asm', 1: 'ben', 2: 'brx', 3: 'doi', 4: 'eng', 5: 'gom',\n",
    "    6: 'guj', 7: 'hin', 8: 'kan', 9: 'kas', 10: 'mai', 11: 'mal',\n",
    "    12: 'mar', 13: 'mni_Beng', 14: 'mni_Mtei', 15: 'npi', 16: 'ory',\n",
    "    17: 'pan', 18: 'san', 19: 'sat', 20: 'snd_Arab', 21: 'snd_Deva',\n",
    "    22: 'tam', 23: 'tel', 24: 'urd'\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenize a sentence using SSF tokenizer\n",
    "# -------------------------------\n",
    "def label_and_transliterate_tokens(tokens, main_lang):\n",
    "    labeled = []\n",
    "\n",
    "    for i, t in enumerate(tokens):\n",
    "        # ---------------------------\n",
    "        # 1. Punctuation\n",
    "        # ---------------------------\n",
    "        if all(char in '.,!?()[]{}:;\\'\"‚Äú‚Äù‚Äò‚Äô' for char in t):\n",
    "            labeled.append({\"token\": t, \"lang\": \"punct\"})\n",
    "            continue\n",
    "\n",
    "        # ---------------------------\n",
    "        # 2. If token inside () and ASCII & <=3 letters ‚Üí keep raw\n",
    "        # ---------------------------\n",
    "        if (\n",
    "            t.startswith(\"(\") and t.endswith(\")\") and\n",
    "            all(ord(c) < 128 for c in t.strip(\"()\")) and\n",
    "            len(t.strip(\"()\")) <= 3\n",
    "        ):\n",
    "            labeled.append({\"token\": t.strip(\"()\"), \"lang\": \"eng\"})\n",
    "            continue\n",
    "\n",
    "        # ---------------------------\n",
    "        # 3. If ASCII token ‚Üí transliterate to main_lang\n",
    "        # ---------------------------\n",
    "        if all(ord(c) < 128 for c in t):\n",
    "            try:\n",
    "                trn = Transliterator(source='eng', target=main_lang, build_lookup=True)\n",
    "                translit = trn.transform(t)\n",
    "                labeled.append({\"token\": translit, \"lang\": main_lang})\n",
    "            except NotImplementedError:\n",
    "                labeled.append({\"token\": t, \"lang\": \"eng\"})\n",
    "            continue\n",
    "\n",
    "        # ---------------------------\n",
    "        # 4. If Non-ASCII (Indic etc.)\n",
    "        # ---------------------------\n",
    "        detected_lang = \"hin\"  # üîπ you can replace with actual token-level detector\n",
    "        if detected_lang != main_lang:\n",
    "            try:\n",
    "                trn = Transliterator(source=detected_lang, target=main_lang, build_lookup=True)\n",
    "                translit = trn.transform(t)\n",
    "                labeled.append({\"token\": translit, \"lang\": main_lang})\n",
    "            except NotImplementedError:\n",
    "                labeled.append({\"token\": t, \"lang\": detected_lang})\n",
    "        else:\n",
    "            labeled.append({\"token\": t, \"lang\": main_lang})\n",
    "\n",
    "    return labeled\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "input_sentences = df[\"Input sentence\"].tolist()\n",
    "output_sentences = df[\"Output sentence\"].tolist()\n",
    "\n",
    "input_tokens_list = []\n",
    "output_tokens_list = []\n",
    "\n",
    "# -------------------------------\n",
    "# Process each sentence row\n",
    "# -------------------------------\n",
    "for inp_sent, out_sent in zip(input_sentences, output_sentences):\n",
    "    # Sentence-level language\n",
    "    inp_pred = pipe([inp_sent])[0]['label']\n",
    "    inp_lang = index_to_label_dict[int(inp_pred.split('_')[1])]\n",
    "\n",
    "    out_pred = pipe([out_sent])[0]['label']\n",
    "    out_lang = index_to_label_dict[int(out_pred.split('_')[1])]\n",
    "\n",
    "    # Tokenize\n",
    "    inp_tokens = tokenize_sentence_ssf(inp_sent)\n",
    "    out_tokens = tokenize_sentence_ssf(out_sent)\n",
    "\n",
    "    # Assign token-level language & transliterate\n",
    "    input_tokens_list.append(label_and_transliterate_tokens(inp_tokens, inp_lang))\n",
    "    output_tokens_list.append(label_and_transliterate_tokens(out_tokens, out_lang))\n",
    "\n",
    "# -------------------------------\n",
    "# Example row\n",
    "# -------------------------------\n",
    "row_index = 136\n",
    "print(\"137th Input sentence:\", input_sentences[row_index])\n",
    "print(\"Tokens with language:\", input_tokens_list[row_index])\n",
    "print(\"\\n137th Output sentence:\", output_sentences[row_index])\n",
    "print(\"Tokens with language:\", output_tokens_list[row_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeca99",
   "metadata": {},
   "source": [
    "# Experiment 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d97ef2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Imports\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "from indictrans import Transliterator  # Python class directly\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "csv_path = \"../IndicGEC2025/Hindi/train.csv\"\n",
    "ssf_tokenizer_script = \"../Tokenizer_for_Indian_Languages/tokenize_in_SSF_format_with_sentence_tokenization.py\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load ILID model for sentence-level language identification\n",
    "# -------------------------------\n",
    "tokenizer_model = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pruthwik/ilid-muril-model\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "index_to_label_dict = {\n",
    "    0: 'asm', 1: 'ben', 2: 'brx', 3: 'doi', 4: 'eng', 5: 'gom',\n",
    "    6: 'guj', 7: 'hin', 8: 'kan', 9: 'kas', 10: 'mai', 11: 'mal',\n",
    "    12: 'mar', 13: 'mni_Beng', 14: 'mni_Mtei', 15: 'npi', 16: 'ory',\n",
    "    17: 'pan', 18: 'san', 19: 'sat', 20: 'snd_Arab', 21: 'snd_Deva',\n",
    "    22: 'tam', 23: 'tel', 24: 'urd'\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenize a sentence using SSF tokenizer\n",
    "# -------------------------------\n",
    "def tokenize_sentence_ssf(sentence):\n",
    "    with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(sentence.strip() + \"\\n\")\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, ssf_tokenizer_script,\n",
    "         \"--input\", \"temp_input.txt\",\n",
    "         \"--output\", \"temp_output.txt\",\n",
    "         \"--lang\", \"hi\"],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    tokens = []\n",
    "    with open(\"temp_output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"(\"):\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                tokens.append(parts[1])\n",
    "    return tokens\n",
    "\n",
    "# -------------------------------\n",
    "# Assign token-level language & transliterate\n",
    "# -------------------------------\n",
    "def label_and_transliterate_tokens(tokens, main_lang):\n",
    "    labeled = []\n",
    "\n",
    "    for t in tokens:\n",
    "        # 1. Punctuation\n",
    "        if all(char in '.,!?()[]{}:;\\'\"‚Äú‚Äù‚Äò‚Äô' for char in t):\n",
    "            labeled.append({\"token\": t, \"lang\": \"punct\"})\n",
    "            continue\n",
    "\n",
    "        # 2. If token inside () and ASCII & <=3 letters ‚Üí keep raw\n",
    "        if (\n",
    "            t.startswith(\"(\") and t.endswith(\")\") and\n",
    "            all(ord(c) < 128 for c in t.strip(\"()\")) and\n",
    "            len(t.strip(\"()\")) <= 3\n",
    "        ):\n",
    "            labeled.append({\"token\": t.strip(\"()\"), \"lang\": \"eng\"})\n",
    "            continue\n",
    "\n",
    "        # 3. If ASCII token ‚Üí transliterate to main_lang\n",
    "        if all(ord(c) < 128 for c in t):\n",
    "            try:\n",
    "                trn = Transliterator(source='eng', target=main_lang, build_lookup=True)\n",
    "                translit = trn.transform(t)\n",
    "                labeled.append({\"token\": translit, \"lang\": main_lang})\n",
    "            except NotImplementedError:\n",
    "                labeled.append({\"token\": t, \"lang\": \"eng\"})\n",
    "            continue\n",
    "\n",
    "        # 4. If Non-ASCII token\n",
    "        detected_lang = \"hin\"  # placeholder (replace with token-level model if needed)\n",
    "        if detected_lang != main_lang:\n",
    "            try:\n",
    "                trn = Transliterator(source=detected_lang, target=main_lang, build_lookup=True)\n",
    "                translit = trn.transform(t)\n",
    "                labeled.append({\"token\": translit, \"lang\": main_lang})\n",
    "            except NotImplementedError:\n",
    "                labeled.append({\"token\": t, \"lang\": detected_lang})\n",
    "        else:\n",
    "            labeled.append({\"token\": t, \"lang\": main_lang})\n",
    "\n",
    "    return labeled\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "input_sentences = df[\"Input sentence\"].tolist()\n",
    "output_sentences = df[\"Output sentence\"].tolist()\n",
    "\n",
    "input_tokens_list = []\n",
    "output_tokens_list = []\n",
    "\n",
    "# -------------------------------\n",
    "# Process each sentence row\n",
    "# -------------------------------\n",
    "for inp_sent, out_sent in zip(input_sentences, output_sentences):\n",
    "    # Sentence-level language\n",
    "    inp_pred = pipe([inp_sent])[0]['label']\n",
    "    inp_lang = index_to_label_dict[int(inp_pred.split('_')[1])]\n",
    "\n",
    "    out_pred = pipe([out_sent])[0]['label']\n",
    "    out_lang = index_to_label_dict[int(out_pred.split('_')[1])]\n",
    "\n",
    "    # Tokenize\n",
    "    inp_tokens = tokenize_sentence_ssf(inp_sent)\n",
    "    out_tokens = tokenize_sentence_ssf(out_sent)\n",
    "\n",
    "    # Assign token-level language & transliterate\n",
    "    input_tokens_list.append(label_and_transliterate_tokens(inp_tokens, inp_lang))\n",
    "    output_tokens_list.append(label_and_transliterate_tokens(out_tokens, out_lang))\n",
    "\n",
    "# -------------------------------\n",
    "# Compare tokens ‚Üí assign grammar error label\n",
    "# -------------------------------\n",
    "out_rows = []\n",
    "for inp_sent, out_sent, inp_tokens, out_tokens in zip(input_sentences, output_sentences, input_tokens_list, output_tokens_list):\n",
    "    inp_raw = [tok[\"token\"] for tok in inp_tokens]\n",
    "    out_raw = [tok[\"token\"] for tok in out_tokens]\n",
    "\n",
    "    if len(inp_raw) != len(out_raw):\n",
    "        label = 1\n",
    "    else:\n",
    "        if all(i == o for i, o in zip(inp_raw, out_raw)):\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "\n",
    "    out_rows.append({\n",
    "        \"Input sentence\": inp_sent,\n",
    "        \"Output sentence\": out_sent,\n",
    "        \"has_grammar_error\": label\n",
    "    })\n",
    "\n",
    "# -------------------------------\n",
    "# Save new CSV\n",
    "# -------------------------------\n",
    "output_csv_path = os.path.join(os.path.dirname(\"/\"), \"labeled_sentences.csv\")\n",
    "pd.DataFrame(out_rows).to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Labeled CSV saved to: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
