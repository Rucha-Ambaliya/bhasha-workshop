{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6525,
     "status": "ok",
     "timestamp": 1759819137139,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "_cthpaQX7bLA",
    "outputId": "b22182db-6cdf-43e5-8f70-8d57a74f48d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13264,
     "status": "ok",
     "timestamp": 1759819150410,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "sS-M7hCA8bX_",
    "outputId": "3c4c44a7-c9be-40d9-8b6d-5ec1ca27927c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 10000, Errors Created: 4996 (49.96%)\n",
      "\n",
      "ðŸ“˜ Language: HINDI (hin_Deva)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Analyzer: word, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.96        | 50.55         | 0.5309 | 0.5309  | 0.5309 | 0.5309\n",
      "LinearSVC            | 49.96        | 49.34         | 0.5266 | 0.5266  | 0.5266 | 0.5266\n",
      "BernoulliNB          | 49.96        | 45.57         | 0.5237 | 0.5237  | 0.5237 | 0.5237\n",
      "\n",
      "Analyzer: word, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.96        | 51.69         | 0.5309 | 0.5309  | 0.5309 | 0.5309\n",
      "LinearSVC            | 49.96        | 50.44         | 0.5304 | 0.5304  | 0.5304 | 0.5304\n",
      "BernoulliNB          | 49.96        | 45.58         | 0.5194 | 0.5194  | 0.5194 | 0.5194\n",
      "\n",
      "Analyzer: char, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.96        | 47.70         | 0.5084 | 0.5084  | 0.5084 | 0.5084\n",
      "LinearSVC            | 49.96        | 48.43         | 0.5067 | 0.5067  | 0.5067 | 0.5067\n",
      "BernoulliNB          | 49.96        | 47.07         | 0.5063 | 0.5063  | 0.5063 | 0.5063\n",
      "\n",
      "Analyzer: char, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.96        | 48.22         | 0.5294 | 0.5294  | 0.5294 | 0.5294\n",
      "LinearSVC            | 49.96        | 46.74         | 0.5444 | 0.5444  | 0.5444 | 0.5444\n",
      "BernoulliNB          | 49.96        | 50.95         | 0.5527 | 0.5527  | 0.5527 | 0.5527\n",
      "\n",
      "âœ… Best Model for Hindi:\n",
      "Classifier: BernoulliNB, Analyzer: char, Ngram: (1, 2), F1: 0.5527, Errors Detected Correctly: 5095/4996 (50.95%)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell 2: Hindi Classification\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Commented, can be used later\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load Hindi data\n",
    "csv_file = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label/hindi/Data_fetch_aug_label.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df[\"Input Sentences\"]\n",
    "labels = df[\"Grammatical Error\"]\n",
    "total_sentences = len(labels)\n",
    "created_errors = (labels == 1).sum()\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=1)  # Uncomment to use Random Forest\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "print(f\"\\nTotal Sentences: {total_sentences}, Errors Created: {created_errors} ({created_errors/total_sentences*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“˜ Language: HINDI (hin_Deva)\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    for ngram_range in ngrams:\n",
    "        token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "        tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "        X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "        print(f\"\\nAnalyzer: {analyzer}, Ngram: {ngram_range}\")\n",
    "        print(f\"{'Classifier':<20} | {'Err Created %':<12} | {'Err Detected %':<13} | F1    | Precision | Recall | Accuracy\")\n",
    "        print(\"-\"*90)\n",
    "\n",
    "        for cls_name, cls_model in classifiers.items():\n",
    "            y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "            f1 = f1_score(labels, y_pred, average='micro')\n",
    "            precision = precision_score(labels, y_pred, average='micro')\n",
    "            recall = recall_score(labels, y_pred, average='micro')\n",
    "            accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "            detected_errors = (y_pred == 1).sum()\n",
    "            err_created_pct = created_errors / total_sentences * 100\n",
    "            err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "            print(f\"{cls_name:<20} | {err_created_pct:<12.2f} | {err_detected_pct:<13.2f} | \"\n",
    "                  f\"{f1:.4f} | {precision:.4f}  | {recall:.4f} | {accuracy:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = (cls_name, analyzer, ngram_range, f1, err_detected_pct, detected_errors)\n",
    "\n",
    "print(\"\\nâœ… Best Model for Hindi:\")\n",
    "print(f\"Classifier: {best_model_info[0]}, Analyzer: {best_model_info[1]}, Ngram: {best_model_info[2]}, \"\n",
    "      f\"F1: {best_model_info[3]:.4f}, Errors Detected Correctly: {best_model_info[5]}/{created_errors} \"\n",
    "      f\"({best_model_info[4]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15038,
     "status": "ok",
     "timestamp": 1759819165465,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "kt1jbYY9ZNZ6",
    "outputId": "b2bb2f6a-df04-4736-f52f-7fb9396342e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 10000, Errors Created: 4997 (49.97%)\n",
      "\n",
      "ðŸ“˜ Language: BANGLA (ben_Beng)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Analyzer: word, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.97        | 51.24         | 0.5127 | 0.5127  | 0.5127 | 0.5127\n",
      "LinearSVC            | 49.97        | 50.34         | 0.5085 | 0.5085  | 0.5085 | 0.5085\n",
      "BernoulliNB          | 49.97        | 45.49         | 0.4964 | 0.4964  | 0.4964 | 0.4964\n",
      "\n",
      "Analyzer: word, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.97        | 51.36         | 0.5093 | 0.5093  | 0.5093 | 0.5093\n",
      "LinearSVC            | 49.97        | 50.84         | 0.5123 | 0.5123  | 0.5123 | 0.5123\n",
      "BernoulliNB          | 49.97        | 44.09         | 0.5048 | 0.5048  | 0.5048 | 0.5048\n",
      "\n",
      "Analyzer: char, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.97        | 47.99         | 0.4956 | 0.4956  | 0.4956 | 0.4956\n",
      "LinearSVC            | 49.97        | 48.25         | 0.4944 | 0.4944  | 0.4944 | 0.4944\n",
      "BernoulliNB          | 49.97        | 49.75         | 0.4990 | 0.4990  | 0.4990 | 0.4990\n",
      "\n",
      "Analyzer: char, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.97        | 48.00         | 0.5307 | 0.5307  | 0.5307 | 0.5307\n",
      "LinearSVC            | 49.97        | 46.69         | 0.5406 | 0.5406  | 0.5406 | 0.5406\n",
      "BernoulliNB          | 49.97        | 48.69         | 0.5456 | 0.5456  | 0.5456 | 0.5456\n",
      "\n",
      "âœ… Best Model for Bangla:\n",
      "Classifier: BernoulliNB, Analyzer: char, Ngram: (1, 2), F1: 0.5456, Errors Detected Correctly: 4869/4997 (48.69%)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell: Bangla Classification\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Commented, can be used later\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load Bangla data\n",
    "csv_file = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label/bangla/Data_fetch_aug_label.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df[\"Input Sentences\"]\n",
    "labels = df[\"Grammatical Error\"]\n",
    "total_sentences = len(labels)\n",
    "created_errors = (labels == 1).sum()\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=1)  # Uncomment to use Random Forest\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "print(f\"\\nTotal Sentences: {total_sentences}, Errors Created: {created_errors} ({created_errors/total_sentences*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“˜ Language: BANGLA (ben_Beng)\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    for ngram_range in ngrams:\n",
    "        token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "        tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "        X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "        print(f\"\\nAnalyzer: {analyzer}, Ngram: {ngram_range}\")\n",
    "        print(f\"{'Classifier':<20} | {'Err Created %':<12} | {'Err Detected %':<13} | F1    | Precision | Recall | Accuracy\")\n",
    "        print(\"-\"*90)\n",
    "\n",
    "        for cls_name, cls_model in classifiers.items():\n",
    "            y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "            f1 = f1_score(labels, y_pred, average='micro')\n",
    "            precision = precision_score(labels, y_pred, average='micro')\n",
    "            recall = recall_score(labels, y_pred, average='micro')\n",
    "            accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "            detected_errors = (y_pred == 1).sum()\n",
    "            err_created_pct = created_errors / total_sentences * 100\n",
    "            err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "            print(f\"{cls_name:<20} | {err_created_pct:<12.2f} | {err_detected_pct:<13.2f} | \"\n",
    "                  f\"{f1:.4f} | {precision:.4f}  | {recall:.4f} | {accuracy:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = (cls_name, analyzer, ngram_range, f1, err_detected_pct, detected_errors)\n",
    "\n",
    "print(\"\\nâœ… Best Model for Bangla:\")\n",
    "print(f\"Classifier: {best_model_info[0]}, Analyzer: {best_model_info[1]}, Ngram: {best_model_info[2]}, \"\n",
    "      f\"F1: {best_model_info[3]:.4f}, Errors Detected Correctly: {best_model_info[5]}/{created_errors} \"\n",
    "      f\"({best_model_info[4]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13528,
     "status": "ok",
     "timestamp": 1759819264370,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "un2IXV00Zq2v",
    "outputId": "cbf15c65-dafe-4058-9cf6-91a02a3ae40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 10000, Errors Created: 5000 (50.00%)\n",
      "\n",
      "ðŸ“˜ Language: MALAYALAM (mal_Mlym)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Analyzer: word, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 50.00        | 51.87         | 0.4985 | 0.4985  | 0.4985 | 0.4985\n",
      "LinearSVC            | 50.00        | 50.50         | 0.4976 | 0.4976  | 0.4976 | 0.4976\n",
      "BernoulliNB          | 50.00        | 48.69         | 0.4985 | 0.4985  | 0.4985 | 0.4985\n",
      "\n",
      "Analyzer: word, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 50.00        | 52.07         | 0.5003 | 0.5003  | 0.5003 | 0.5003\n",
      "LinearSVC            | 50.00        | 51.01         | 0.4975 | 0.4975  | 0.4975 | 0.4975\n",
      "BernoulliNB          | 50.00        | 49.44         | 0.4954 | 0.4954  | 0.4954 | 0.4954\n",
      "\n",
      "Analyzer: char, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 50.00        | 47.32         | 0.5032 | 0.5032  | 0.5032 | 0.5032\n",
      "LinearSVC            | 50.00        | 47.67         | 0.5035 | 0.5035  | 0.5035 | 0.5035\n",
      "BernoulliNB          | 50.00        | 51.69         | 0.5073 | 0.5073  | 0.5073 | 0.5073\n",
      "\n",
      "Analyzer: char, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 50.00        | 48.06         | 0.5404 | 0.5404  | 0.5404 | 0.5404\n",
      "LinearSVC            | 50.00        | 47.25         | 0.5547 | 0.5547  | 0.5547 | 0.5547\n",
      "BernoulliNB          | 50.00        | 55.21         | 0.5559 | 0.5559  | 0.5559 | 0.5559\n",
      "\n",
      "âœ… Best Model for Malayalam:\n",
      "Classifier: BernoulliNB, Analyzer: char, Ngram: (1, 2), F1: 0.5559, Errors Detected Correctly: 5521/5000 (55.21%)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell: Malayalam Classification\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Commented, can be used later\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load Malayalam data\n",
    "csv_file = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label/malayalam/Data_fetch_aug_label.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df[\"Input Sentences\"]\n",
    "labels = df[\"Grammatical Error\"]\n",
    "total_sentences = len(labels)\n",
    "created_errors = (labels == 1).sum()\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=1)  # Uncomment to use Random Forest\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "print(f\"\\nTotal Sentences: {total_sentences}, Errors Created: {created_errors} ({created_errors/total_sentences*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“˜ Language: MALAYALAM (mal_Mlym)\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    for ngram_range in ngrams:\n",
    "        token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "        tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "        X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "        print(f\"\\nAnalyzer: {analyzer}, Ngram: {ngram_range}\")\n",
    "        print(f\"{'Classifier':<20} | {'Err Created %':<12} | {'Err Detected %':<13} | F1    | Precision | Recall | Accuracy\")\n",
    "        print(\"-\"*90)\n",
    "\n",
    "        for cls_name, cls_model in classifiers.items():\n",
    "            y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "            f1 = f1_score(labels, y_pred, average='micro')\n",
    "            precision = precision_score(labels, y_pred, average='micro')\n",
    "            recall = recall_score(labels, y_pred, average='micro')\n",
    "            accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "            detected_errors = (y_pred == 1).sum()\n",
    "            err_created_pct = created_errors / total_sentences * 100\n",
    "            err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "            print(f\"{cls_name:<20} | {err_created_pct:<12.2f} | {err_detected_pct:<13.2f} | \"\n",
    "                  f\"{f1:.4f} | {precision:.4f}  | {recall:.4f} | {accuracy:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = (cls_name, analyzer, ngram_range, f1, err_detected_pct, detected_errors)\n",
    "\n",
    "print(\"\\nâœ… Best Model for Malayalam:\")\n",
    "print(f\"Classifier: {best_model_info[0]}, Analyzer: {best_model_info[1]}, Ngram: {best_model_info[2]}, \"\n",
    "      f\"F1: {best_model_info[3]:.4f}, Errors Detected Correctly: {best_model_info[5]}/{created_errors} \"\n",
    "      f\"({best_model_info[4]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17964,
     "status": "ok",
     "timestamp": 1759819282354,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "11R-8kZBZtp5",
    "outputId": "fe08dea6-ac2f-42d8-e42b-f01701fb010b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 10000, Errors Created: 4995 (49.95%)\n",
      "\n",
      "ðŸ“˜ Language: TAMIL (tam_Taml)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Analyzer: word, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 50.92         | 0.5009 | 0.5009  | 0.5009 | 0.5009\n",
      "LinearSVC            | 49.95        | 49.92         | 0.4997 | 0.4997  | 0.4997 | 0.4997\n",
      "BernoulliNB          | 49.95        | 45.71         | 0.4972 | 0.4972  | 0.4972 | 0.4972\n",
      "\n",
      "Analyzer: word, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 50.67         | 0.5008 | 0.5008  | 0.5008 | 0.5008\n",
      "LinearSVC            | 49.95        | 50.19         | 0.5030 | 0.5030  | 0.5030 | 0.5030\n",
      "BernoulliNB          | 49.95        | 42.96         | 0.4991 | 0.4991  | 0.4991 | 0.4991\n",
      "\n",
      "Analyzer: char, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 47.23         | 0.5144 | 0.5144  | 0.5144 | 0.5144\n",
      "LinearSVC            | 49.95        | 48.00         | 0.5169 | 0.5169  | 0.5169 | 0.5169\n",
      "BernoulliNB          | 49.95        | 41.03         | 0.5082 | 0.5082  | 0.5082 | 0.5082\n",
      "\n",
      "Analyzer: char, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 48.04         | 0.5437 | 0.5437  | 0.5437 | 0.5437\n",
      "LinearSVC            | 49.95        | 46.17         | 0.5632 | 0.5632  | 0.5632 | 0.5632\n",
      "BernoulliNB          | 49.95        | 43.07         | 0.5786 | 0.5786  | 0.5786 | 0.5786\n",
      "\n",
      "âœ… Best Model for Tamil:\n",
      "Classifier: BernoulliNB, Analyzer: char, Ngram: (1, 2), F1: 0.5786, Errors Detected Correctly: 4307/4995 (43.07%)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell: Tamil Classification\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Commented, can be used later\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load Tamil data\n",
    "csv_file = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label/tamil/Data_fetch_aug_label.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df[\"Input Sentences\"]\n",
    "labels = df[\"Grammatical Error\"]\n",
    "total_sentences = len(labels)\n",
    "created_errors = (labels == 1).sum()\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=1)  # Uncomment to use Random Forest\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "print(f\"\\nTotal Sentences: {total_sentences}, Errors Created: {created_errors} ({created_errors/total_sentences*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“˜ Language: TAMIL (tam_Taml)\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    for ngram_range in ngrams:\n",
    "        token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "        tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "        X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "        print(f\"\\nAnalyzer: {analyzer}, Ngram: {ngram_range}\")\n",
    "        print(f\"{'Classifier':<20} | {'Err Created %':<12} | {'Err Detected %':<13} | F1    | Precision | Recall | Accuracy\")\n",
    "        print(\"-\"*90)\n",
    "\n",
    "        for cls_name, cls_model in classifiers.items():\n",
    "            y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "            f1 = f1_score(labels, y_pred, average='micro')\n",
    "            precision = precision_score(labels, y_pred, average='micro')\n",
    "            recall = recall_score(labels, y_pred, average='micro')\n",
    "            accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "            detected_errors = (y_pred == 1).sum()\n",
    "            err_created_pct = created_errors / total_sentences * 100\n",
    "            err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "            print(f\"{cls_name:<20} | {err_created_pct:<12.2f} | {err_detected_pct:<13.2f} | \"\n",
    "                  f\"{f1:.4f} | {precision:.4f}  | {recall:.4f} | {accuracy:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = (cls_name, analyzer, ngram_range, f1, err_detected_pct, detected_errors)\n",
    "\n",
    "print(\"\\nâœ… Best Model for Tamil:\")\n",
    "print(f\"Classifier: {best_model_info[0]}, Analyzer: {best_model_info[1]}, Ngram: {best_model_info[2]}, \"\n",
    "      f\"F1: {best_model_info[3]:.4f}, Errors Detected Correctly: {best_model_info[5]}/{created_errors} \"\n",
    "      f\"({best_model_info[4]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19534,
     "status": "ok",
     "timestamp": 1759819305602,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "9Nx3zVFDZzrv",
    "outputId": "1b141a82-4ab5-43a9-b701-79d659dff2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 10000, Errors Created: 4995 (49.95%)\n",
      "\n",
      "ðŸ“˜ Language: TELUGU (tel_Telu)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Analyzer: word, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 51.52         | 0.5041 | 0.5041  | 0.5041 | 0.5041\n",
      "LinearSVC            | 49.95        | 50.31         | 0.4994 | 0.4994  | 0.4994 | 0.4994\n",
      "BernoulliNB          | 49.95        | 45.90         | 0.4991 | 0.4991  | 0.4991 | 0.4991\n",
      "\n",
      "Analyzer: word, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 51.99         | 0.5032 | 0.5032  | 0.5032 | 0.5032\n",
      "LinearSVC            | 49.95        | 50.59         | 0.5034 | 0.5034  | 0.5034 | 0.5034\n",
      "BernoulliNB          | 49.95        | 44.69         | 0.5048 | 0.5048  | 0.5048 | 0.5048\n",
      "\n",
      "Analyzer: char, Ngram: (1, 1)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 46.67         | 0.5084 | 0.5084  | 0.5084 | 0.5084\n",
      "LinearSVC            | 49.95        | 47.23         | 0.5062 | 0.5062  | 0.5062 | 0.5062\n",
      "BernoulliNB          | 49.95        | 49.87         | 0.5008 | 0.5008  | 0.5008 | 0.5008\n",
      "\n",
      "Analyzer: char, Ngram: (1, 2)\n",
      "Classifier           | Err Created % | Err Detected % | F1    | Precision | Recall | Accuracy\n",
      "------------------------------------------------------------------------------------------\n",
      "LogisticRegression   | 49.95        | 48.66         | 0.5331 | 0.5331  | 0.5331 | 0.5331\n",
      "LinearSVC            | 49.95        | 47.06         | 0.5525 | 0.5525  | 0.5525 | 0.5525\n",
      "BernoulliNB          | 49.95        | 49.67         | 0.5644 | 0.5644  | 0.5644 | 0.5644\n",
      "\n",
      "âœ… Best Model for Telugu:\n",
      "Classifier: BernoulliNB, Analyzer: char, Ngram: (1, 2), F1: 0.5644, Errors Detected Correctly: 4967/4995 (49.67%)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell: Telugu Classification\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier  # Commented, can be used later\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load Telugu data\n",
    "csv_file = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label/telugu/Data_fetch_aug_label.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df[\"Input Sentences\"]\n",
    "labels = df[\"Grammatical Error\"]\n",
    "total_sentences = len(labels)\n",
    "created_errors = (labels == 1).sum()\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=1)  # Uncomment to use Random Forest\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "print(f\"\\nTotal Sentences: {total_sentences}, Errors Created: {created_errors} ({created_errors/total_sentences*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“˜ Language: TELUGU (tel_Telu)\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    for ngram_range in ngrams:\n",
    "        token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "        tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "        X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "        print(f\"\\nAnalyzer: {analyzer}, Ngram: {ngram_range}\")\n",
    "        print(f\"{'Classifier':<20} | {'Err Created %':<12} | {'Err Detected %':<13} | F1    | Precision | Recall | Accuracy\")\n",
    "        print(\"-\"*90)\n",
    "\n",
    "        for cls_name, cls_model in classifiers.items():\n",
    "            y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "            f1 = f1_score(labels, y_pred, average='micro')\n",
    "            precision = precision_score(labels, y_pred, average='micro')\n",
    "            recall = recall_score(labels, y_pred, average='micro')\n",
    "            accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "            detected_errors = (y_pred == 1).sum()\n",
    "            err_created_pct = created_errors / total_sentences * 100\n",
    "            err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "            print(f\"{cls_name:<20} | {err_created_pct:<12.2f} | {err_detected_pct:<13.2f} | \"\n",
    "                  f\"{f1:.4f} | {precision:.4f}  | {recall:.4f} | {accuracy:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = (cls_name, analyzer, ngram_range, f1, err_detected_pct, detected_errors)\n",
    "\n",
    "print(\"\\nâœ… Best Model for Telugu:\")\n",
    "print(f\"Classifier: {best_model_info[0]}, Analyzer: {best_model_info[1]}, Ngram: {best_model_info[2]}, \"\n",
    "      f\"F1: {best_model_info[3]:.4f}, Errors Detected Correctly: {best_model_info[5]}/{created_errors} \"\n",
    "      f\"({best_model_info[4]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72930,
     "status": "ok",
     "timestamp": 1759820017802,
     "user": {
      "displayName": "Mahika Dugar",
      "userId": "08588630143455963978"
     },
     "user_tz": -330
    },
    "id": "ur7sQalGb2A2",
    "outputId": "684865f5-f97b-4b98-9761-412bc1941e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Compact Summary of Best Models for All Languages\n",
      "-----------------------------------------------------------\n",
      "Language   | Classifier           | Analyzer | Ngram   | F1    | Precision | Recall | Accuracy | Errors Detected\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Bangla     | BernoulliNB          | char   | (1, 2)  | 0.5456 | 0.5456  | 0.5456 | 0.5456 | 4869/4997 (48.69%)\n",
      "Hindi      | BernoulliNB          | char   | (1, 2)  | 0.5527 | 0.5527  | 0.5527 | 0.5527 | 5095/4996 (50.95%)\n",
      "Malayalam  | BernoulliNB          | char   | (1, 2)  | 0.5559 | 0.5559  | 0.5559 | 0.5559 | 5521/5000 (55.21%)\n",
      "Tamil      | BernoulliNB          | char   | (1, 2)  | 0.5786 | 0.5786  | 0.5786 | 0.5786 | 4307/4995 (43.07%)\n",
      "Telugu     | BernoulliNB          | char   | (1, 2)  | 0.5644 | 0.5644  | 0.5644 | 0.5644 | 4967/4995 (49.67%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Cell: All Languages Classification\n",
    "# -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & Languages\n",
    "# -----------------------------\n",
    "BASE_PATH = '/content/drive/MyDrive/svnit_shared_task/shared_task/bhasha-workshop/Task1_I/Data/IndicCorp_fetch_aug_label'\n",
    "\n",
    "languages = {\n",
    "    \"Bangla\": \"bangla\",\n",
    "    \"Hindi\": \"hindi\",\n",
    "    \"Malayalam\": \"malayalam\",\n",
    "    \"Tamil\": \"tamil\",\n",
    "    \"Telugu\": \"telugu\"\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=1),\n",
    "    \"LinearSVC\": LinearSVC(random_state=1),\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=0.001),\n",
    "    #\"RandomForest\": RandomForestClassifier(random_state=1)  # optional\n",
    "}\n",
    "\n",
    "analyzers = [\"word\", \"char\"]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "\n",
    "# -----------------------------\n",
    "# Summary table\n",
    "# -----------------------------\n",
    "summary_rows = []\n",
    "\n",
    "for lang_display, lang_code in languages.items():\n",
    "    csv_file = os.path.join(BASE_PATH, lang_code, 'Data_fetch_aug_label.csv')\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"âŒ File not found: {csv_file}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    texts = df[\"Input Sentences\"]\n",
    "    labels = df[\"Grammatical Error\"]\n",
    "    total_sentences = len(labels)\n",
    "    created_errors = (labels == 1).sum()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_info = None\n",
    "\n",
    "    for analyzer in analyzers:\n",
    "        for ngram_range in ngrams:\n",
    "            token_pattern = r'\\S+' if analyzer=='word' else None\n",
    "            tfidf = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, token_pattern=token_pattern)\n",
    "            X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "            for cls_name, cls_model in classifiers.items():\n",
    "                y_pred = cross_val_predict(cls_model, X_tfidf, labels, cv=5)\n",
    "\n",
    "                f1 = f1_score(labels, y_pred, average='micro')\n",
    "                precision = precision_score(labels, y_pred, average='micro')\n",
    "                recall = recall_score(labels, y_pred, average='micro')\n",
    "                accuracy = accuracy_score(labels, y_pred)\n",
    "                detected_errors = (y_pred == 1).sum()\n",
    "                err_detected_pct = detected_errors / total_sentences * 100\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_info = {\n",
    "                        \"Language\": lang_display,\n",
    "                        \"Classifier\": cls_name,\n",
    "                        \"Analyzer\": analyzer,\n",
    "                        \"Ngram\": ngram_range,\n",
    "                        \"F1\": f1,\n",
    "                        \"Precision\": precision,\n",
    "                        \"Recall\": recall,\n",
    "                        \"Accuracy\": accuracy,\n",
    "                        \"Errors Detected\": f\"{detected_errors}/{created_errors} ({err_detected_pct:.2f}%)\"\n",
    "                    }\n",
    "\n",
    "    summary_rows.append(best_info)\n",
    "\n",
    "# -----------------------------\n",
    "# Print compact summary\n",
    "# -----------------------------\n",
    "print(\"\\nâœ… Compact Summary of Best Models for All Languages\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"{'Language':<10} | {'Classifier':<20} | {'Analyzer':<6} | {'Ngram':<7} | F1    | Precision | Recall | Accuracy | Errors Detected\")\n",
    "print(\"-\"*110)\n",
    "for row in summary_rows:\n",
    "    print(f\"{row['Language']:<10} | {row['Classifier']:<20} | {row['Analyzer']:<6} | {row['Ngram']!s:<7} | \"\n",
    "          f\"{row['F1']:.4f} | {row['Precision']:.4f}  | {row['Recall']:.4f} | {row['Accuracy']:.4f} | {row['Errors Detected']}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOdVXQKqUhe5ngNHCNFRIy9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
